{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5. 콜백 사용하기.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R0Cev5e059_k"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "housing = fetch_california_housing()\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "EAMavWAp6OqD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])    "
      ],
      "metadata": {
        "id": "a9eeZtvc6PHS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ModelCheckpoint 콜백 : 예상치 못한 사고로 인해 학습이 마무리되지 못했을 경우를 대비하여 매 에포크마다 모델 파일 생성\n",
        "\n",
        "### save_best_only 파라미터를 True로 넘겨주어 가장 좋은 결과를 보였던 훈련 회차의 결과만 저장되도록 할 수도 있음.\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb])\n",
        "model = keras.models.load_model(\"my_keras_model.h5\")\n",
        "mse_test = model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5AyakCz6Rxe",
        "outputId": "037da05d-4ac5-4483-e2bf-255e2ce66c74"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "363/363 [==============================] - 3s 6ms/step - loss: 2.3864 - val_loss: 1.1804\n",
            "Epoch 2/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.8160 - val_loss: 0.7182\n",
            "Epoch 3/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6655 - val_loss: 0.6677\n",
            "Epoch 4/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6298 - val_loss: 0.6440\n",
            "Epoch 5/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6079 - val_loss: 0.6270\n",
            "Epoch 6/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5914 - val_loss: 0.6109\n",
            "Epoch 7/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5765 - val_loss: 0.5994\n",
            "Epoch 8/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5650 - val_loss: 0.5863\n",
            "Epoch 9/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5530 - val_loss: 0.5751\n",
            "Epoch 10/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5434 - val_loss: 0.5654\n",
            "162/162 [==============================] - 0s 1ms/step - loss: 0.5571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EarlyStopping 콜백 : 일정 에포크동안 검증 세트에 대한 점수가 향상되지 않는 경우 훈련을 조기 종료.\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                  restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=200,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "mse_test = model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yJ6vu5M7N_1",
        "outputId": "1fc6044f-8643-4dad-d2fe-172bb1af13d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "363/363 [==============================] - 3s 5ms/step - loss: 0.3160 - val_loss: 0.3397\n",
            "Epoch 2/200\n",
            "363/363 [==============================] - 1s 4ms/step - loss: 0.3156 - val_loss: 0.3416\n",
            "Epoch 3/200\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.3153 - val_loss: 0.3423\n",
            "Epoch 4/200\n",
            "363/363 [==============================] - 1s 4ms/step - loss: 0.3161 - val_loss: 0.3429\n",
            "Epoch 5/200\n",
            "363/363 [==============================] - 1s 4ms/step - loss: 0.3168 - val_loss: 0.3420\n",
            "Epoch 6/200\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.3152 - val_loss: 0.3398\n",
            "Epoch 7/200\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.3135 - val_loss: 0.3394\n",
            "Epoch 8/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3142 - val_loss: 0.3396\n",
            "Epoch 9/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3131 - val_loss: 0.3393\n",
            "Epoch 10/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3130 - val_loss: 0.3392\n",
            "Epoch 11/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3140 - val_loss: 0.3399\n",
            "Epoch 12/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3128 - val_loss: 0.3409\n",
            "Epoch 13/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3137 - val_loss: 0.3383\n",
            "Epoch 14/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3121 - val_loss: 0.3385\n",
            "Epoch 15/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3120 - val_loss: 0.3387\n",
            "Epoch 16/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3120 - val_loss: 0.3369\n",
            "Epoch 17/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3125 - val_loss: 0.3385\n",
            "Epoch 18/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3115 - val_loss: 0.3378\n",
            "Epoch 19/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3106 - val_loss: 0.3370\n",
            "Epoch 20/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3107 - val_loss: 0.3378\n",
            "Epoch 21/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3104 - val_loss: 0.3370\n",
            "Epoch 22/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3113 - val_loss: 0.3382\n",
            "Epoch 23/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3127 - val_loss: 0.3368\n",
            "Epoch 24/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3102 - val_loss: 0.3381\n",
            "Epoch 25/200\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.3121 - val_loss: 0.3363\n",
            "Epoch 26/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3108 - val_loss: 0.3372\n",
            "Epoch 27/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3143 - val_loss: 0.3350\n",
            "Epoch 28/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3139 - val_loss: 0.3373\n",
            "Epoch 29/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3095 - val_loss: 0.3375\n",
            "Epoch 30/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3091 - val_loss: 0.3351\n",
            "Epoch 31/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3087 - val_loss: 0.3347\n",
            "Epoch 32/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3101 - val_loss: 0.3348\n",
            "Epoch 33/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3098 - val_loss: 0.3338\n",
            "Epoch 34/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3111 - val_loss: 0.3397\n",
            "Epoch 35/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3112 - val_loss: 0.3349\n",
            "Epoch 36/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3100 - val_loss: 0.3340\n",
            "Epoch 37/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3081 - val_loss: 0.3352\n",
            "Epoch 38/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3081 - val_loss: 0.3395\n",
            "Epoch 39/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3075 - val_loss: 0.3377\n",
            "Epoch 40/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3096 - val_loss: 0.3344\n",
            "Epoch 41/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3069 - val_loss: 0.3328\n",
            "Epoch 42/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3114 - val_loss: 0.3389\n",
            "Epoch 43/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3115 - val_loss: 0.3359\n",
            "Epoch 44/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3090 - val_loss: 0.3356\n",
            "Epoch 45/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3066 - val_loss: 0.3326\n",
            "Epoch 46/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3065 - val_loss: 0.3333\n",
            "Epoch 47/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3100 - val_loss: 0.3323\n",
            "Epoch 48/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3068 - val_loss: 0.3344\n",
            "Epoch 49/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3070 - val_loss: 0.3330\n",
            "Epoch 50/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3122 - val_loss: 0.3329\n",
            "Epoch 51/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3059 - val_loss: 0.3347\n",
            "Epoch 52/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3074 - val_loss: 0.3321\n",
            "Epoch 53/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3049 - val_loss: 0.3310\n",
            "Epoch 54/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3070 - val_loss: 0.3350\n",
            "Epoch 55/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3079 - val_loss: 0.3333\n",
            "Epoch 56/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3039 - val_loss: 0.3343\n",
            "Epoch 57/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3051 - val_loss: 0.3334\n",
            "Epoch 58/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3046 - val_loss: 0.3353\n",
            "Epoch 59/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3090 - val_loss: 0.3328\n",
            "Epoch 60/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3048 - val_loss: 0.3313\n",
            "Epoch 61/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3122 - val_loss: 0.3339\n",
            "Epoch 62/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3059 - val_loss: 0.3326\n",
            "Epoch 63/200\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3090 - val_loss: 0.3452\n",
            "162/162 [==============================] - 0s 1ms/step - loss: 0.3408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 더 많은 제어를 원할 경우 사용자 정의 콜백을 만들 수도 있음.\n",
        "\n",
        "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
      ],
      "metadata": {
        "id": "mMOuMyjI7ppo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}