{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6. 분류와 위치 추정.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CUaVogWaL-Hr"
      },
      "outputs": [],
      "source": [
        "# 파이썬 ≥3.5 필수\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# 사이킷런 ≥0.20 필수\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# 코랩에서 실행되는 노트북인가요?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "# 텐서플로 ≥2.0 필수\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"감지된 GPU가 없습니다. GPU가 없으면 CNN은 매우 느릴 수 있습니다.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"런타임 > 런타임 유형 변경 메뉴를 선택하고 하드웨어 가속기로 GPU를 고르세요.\")\n",
        "\n",
        "# 공통 모듈 임포트\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 노트북 실행 결과를 동일하게 유지하기 위해\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 깔끔한 그래프 출력을 위해\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# 그림을 저장할 위치\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"cnn\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"그림 저장\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import tensorflow_datasets as tfds\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
        "class_names = info.features[\"label\"].names\n",
        "n_classes = info.features[\"label\"].num_classes\n",
        "dataset_size = info.splits[\"train\"].num_examples"
      ],
      "metadata": {
        "id": "s_94_RU4Mn2L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
        "    \"tf_flowers\",\n",
        "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
        "    as_supervised=True)\n",
        "\n",
        "def central_crop(image):\n",
        "    shape = tf.shape(image)\n",
        "    min_dim = tf.reduce_min([shape[0], shape[1]])\n",
        "    top_crop = (shape[0] - min_dim) // 4\n",
        "    bottom_crop = shape[0] - top_crop\n",
        "    left_crop = (shape[1] - min_dim) // 4\n",
        "    right_crop = shape[1] - left_crop\n",
        "    return image[top_crop:bottom_crop, left_crop:right_crop]\n",
        "\n",
        "def random_crop(image):\n",
        "    shape = tf.shape(image)\n",
        "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 90 // 100\n",
        "    return tf.image.random_crop(image, [min_dim, min_dim, 3])\n",
        "\n",
        "def preprocess(image, label, randomize=False):\n",
        "    if randomize:\n",
        "        cropped_image = random_crop(image)\n",
        "        cropped_image = tf.image.random_flip_left_right(cropped_image)\n",
        "    else:\n",
        "        cropped_image = central_crop(image)\n",
        "    resized_image = tf.image.resize(cropped_image, [224, 224])\n",
        "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
        "    return final_image, label\n",
        "\n",
        "batch_size = 32\n",
        "train_set = train_set_raw.shuffle(1000).repeat()\n",
        "train_set = train_set.map(partial(preprocess, randomize=True)).batch(batch_size).prefetch(1)\n",
        "valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)\n",
        "test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)"
      ],
      "metadata": {
        "id": "aLITjOkqM3YE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
        "                                                  include_top=False)\n",
        "\n",
        "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "class_output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
        "loc_output = keras.layers.Dense(4)(avg)\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9, decay=0.01)\n",
        "model = keras.models.Model(inputs=base_model.input,\n",
        "                           outputs=[class_output, loc_output])\n",
        "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n",
        "              loss_weights=[0.8, 0.2],\n",
        "              optimizer=optimizer, metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "U5hr_jWYMAlJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_random_bounding_boxes(images, labels):\n",
        "    fake_bboxes = tf.random.uniform([tf.shape(images)[0], 4])\n",
        "    return images, (labels, fake_bboxes)\n",
        "\n",
        "fake_train_set = train_set.take(5).repeat(2).map(add_random_bounding_boxes)"
      ],
      "metadata": {
        "id": "-X0tl7IkMCDq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(fake_train_set, steps_per_epoch=5, epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I6yvI93Mxpl",
        "outputId": "26430d7d-6b03-44d3-ebb7-d87ea2fc861a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "5/5 [==============================] - 19s 1s/step - loss: 1.1562 - dense_8_loss: 1.3535 - dense_9_loss: 0.3670 - dense_8_accuracy: 0.4250 - dense_9_accuracy: 0.3125\n",
            "Epoch 2/2\n",
            "5/5 [==============================] - 8s 1s/step - loss: 0.6219 - dense_8_loss: 0.7186 - dense_9_loss: 0.2351 - dense_8_accuracy: 0.7437 - dense_9_accuracy: 0.2313\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f43b2119790>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}